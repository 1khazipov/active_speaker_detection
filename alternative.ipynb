{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import moviepy.editor as mp\n",
        "\n",
        "clip = mp.VideoFileClip(r\"drive/MyDrive/studio2/both.mp4\")\n",
        "clip.audio.write_audiofile(r\"drive/MyDrive/studio2/audio.mp3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0L7n4wdhm0K",
        "outputId": "61bc8a17-98c0-4e05-8e25-133dd0155c6f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in drive/MyDrive/studio2/audio.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading libraries"
      ],
      "metadata": {
        "id": "1Ez2ciLewSNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyJYkFghuRmq"
      },
      "outputs": [],
      "source": [
        "!pip install ffpyplayer\n",
        "!pip install pydub\n",
        "!pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip\n",
        "!pip install librosa\n",
        "!git clone https://github.com/ageitgey/face_recognition\n",
        "!pip install face_recognition_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing libraries"
      ],
      "metadata": {
        "id": "Uk8VlF_kxfL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "# from IPython.display import Audio\n",
        "from pyannote.audio import Pipeline\n",
        "import torch\n",
        "import cv2\n",
        "# from ffpyplayer.player import MediaPlayer\n",
        "# from moviepy.editor import AudioFileClip\n",
        "# from moviepy.editor import VideoFileClip\n",
        "from scipy.spatial import distance as dist\n",
        "from imutils.video import FileVideoStream\n",
        "from imutils import face_utils\n",
        "from threading import Thread\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "zhhEr7EXwRhk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "speaker diarization"
      ],
      "metadata": {
        "id": "WumFrtPAxhkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                    use_auth_token=\"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\")\n",
        "pipeline.to(torch.device('cuda'))\n",
        "\n",
        "diarization = pipeline(\"drive/MyDrive/studio2/audio.mp3\", num_speakers=2)\n",
        "\n",
        "with open(\"drive/MyDrive/studio2/diarization.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeUXETRtwiTt",
        "outputId": "ca2a743d-0232-4488-8e9e-e4c9080e1c7a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/studio2/diarization.rttm\") as f:\n",
        "    file = f.read()\n",
        "\n",
        "times = file.split(\"\\n\")[:-1]\n",
        "FPS = 30\n",
        "for i, time in enumerate(times):\n",
        "    new_time = time.split()[3:5]\n",
        "    speaker = time.split()[7]\n",
        "    times[i] = [speaker, [float(new_time[0]), float(new_time[1])]]\n",
        "\n",
        "for i, time in enumerate(times):\n",
        "    frame = [int(time[1][0] * FPS), int((time[1][0] + time[1][1]) * FPS)]\n",
        "    times[i].append(frame)"
      ],
      "metadata": {
        "id": "v9h1AkKByMfz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_speaker0_time, max_speaker1_time = 0, 0\n",
        "max_speaker0_frames, max_speaker1_frames = [], []\n",
        "\n",
        "count_frames = 0\n",
        "for time in times:\n",
        "    if count_frames < max(time[2]):\n",
        "        count_frames = max(time[2])\n",
        "\n",
        "    if time[0] == 'SPEAKER_00' and max_speaker0_time < time[1][1]:\n",
        "        max_speaker0_time = time[1][1]\n",
        "        max_speaker0_frames = time[2]\n",
        "    elif time[0] == 'SPEAKER_01' and max_speaker1_time < time[1][1]:\n",
        "        max_speaker1_time = time[1][1]\n",
        "        max_speaker1_frames = time[2]"
      ],
      "metadata": {
        "id": "B1cdjf5q16yX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "open mouth detection"
      ],
      "metadata": {
        "id": "NbtPG-P-Inif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the key concepts of the code cell **below** were taken from **https**://github.com/mauckc/mouth-open/blob/master/detect_videofile_mouth.py (MIT Licence)"
      ],
      "metadata": {
        "id": "Oodo9JFI0akU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def mouth_aspect_ratio(mouth):\n",
        "#     A = dist.euclidean(mouth[2], mouth[9])\n",
        "#     B = dist.euclidean(mouth[4], mouth[7])\n",
        "#     C = dist.euclidean(mouth[0], mouth[6])\n",
        "#     mar = (A + B) / (2.0 * C)\n",
        "#     return mar\n",
        "\n",
        "\n",
        "# def open_mouth(video, count_frames, speaker0_frames, speaker1_frames):\n",
        "#     ap = argparse.ArgumentParser()\n",
        "#     ap.add_argument('-f')\n",
        "#     ap.add_argument(\"-p\", \"--shape-predictor\",\n",
        "#                     required=False,\n",
        "#                     default='drive/MyDrive/shape_predictor_68_face_landmarks.dat',\n",
        "#                     help=\"path to facial landmark predictor\")\n",
        "#     ap.add_argument(\"-v\", \"--video\", default=video,\n",
        "#                     help=\"video path input\")\n",
        "#     args = vars(ap.parse_args())\n",
        "\n",
        "#     MOUTH_AR_THRESH = 0.6\n",
        "#     detector = dlib.get_frontal_face_detector()\n",
        "#     predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
        "#     (mStart, mEnd) = (49, 68)\n",
        "\n",
        "#     fvs = FileVideoStream(path=args[\"video\"]).start()\n",
        "#     speaker0_frames_number, speaker1_frames_number = 0, 0\n",
        "#     for current_frame in tqdm(range(count_frames)):\n",
        "#         frame = fvs.read()\n",
        "\n",
        "#         if not (speaker0_frames[0] <= current_frame <= speaker0_frames[1] or\n",
        "#                 speaker1_frames[0] <= current_frame <= speaker1_frames[1]):\n",
        "#             continue\n",
        "\n",
        "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#         rects = detector(gray, 0)\n",
        "\n",
        "#         for rect in rects:\n",
        "#             shape = predictor(gray, rect)\n",
        "#             shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "#             mouth = shape[mStart:mEnd]\n",
        "#             mar = mouth_aspect_ratio(mouth)\n",
        "\n",
        "#             if mar > MOUTH_AR_THRESH:\n",
        "#                 if speaker0_frames[0] <= current_frame <= speaker0_frames[1]:\n",
        "#                     speaker0_frames_number += 1\n",
        "\n",
        "#                 if speaker1_frames[0] <= current_frame <= speaker1_frames[1]:\n",
        "#                     speaker1_frames_number += 1\n",
        "\n",
        "#     return (speaker0_frames_number / (speaker0_frames[1] - speaker0_frames[0]),\n",
        "#             speaker1_frames_number / (speaker1_frames[1] - speaker1_frames[0]))"
      ],
      "metadata": {
        "id": "-QEhMG9R0Ysu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# video_0 = 'drive/MyDrive/in_office/speaker_0.mp4'\n",
        "# video_1 = 'drive/MyDrive/in_office/speaker_1.mp4'\n",
        "\n",
        "# video_0_freq0, video_0_freq1 = open_mouth(video_0,\n",
        "#                                           count_frames,\n",
        "#                                           max_speaker0_frames,\n",
        "#                                           max_speaker1_frames)\n",
        "\n",
        "# video_1_freq0, video_1_freq1 = open_mouth(video_1,\n",
        "#                                           count_frames,\n",
        "#                                           max_speaker0_frames,\n",
        "#                                           max_speaker1_frames)\n",
        "\n",
        "# if max(video_0_freq0, video_0_freq1) > max(video_1_freq0, video_1_freq1):\n",
        "#     if video_0_freq0 > video_0_freq1:\n",
        "#         video_0_speaker = 'SPEAKER_00'\n",
        "#         video_1_speaker = 'SPEAKER_01'\n",
        "\n",
        "#     else:\n",
        "#         video_0_speaker = 'SPEAKER_01'\n",
        "#         video_1_speaker = 'SPEAKER_00'\n",
        "\n",
        "# else:\n",
        "#     if video_1_freq0 > video_1_freq1:\n",
        "#         video_0_speaker = 'SPEAKER_01'\n",
        "#         video_1_speaker = 'SPEAKER_00'\n",
        "\n",
        "#     else:\n",
        "#         video_0_speaker = 'SPEAKER_00'\n",
        "#         video_1_speaker = 'SPEAKER_01'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLpAHoV9JUI0",
        "outputId": "7ccac2e7-c05b-43af-a943-5f5931a35188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20429/20429 [08:23<00:00, 40.59it/s] \n",
            "100%|██████████| 20429/20429 [08:22<00:00, 40.68it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# video_0_dicts, video_1_dicts = [], []\n",
        "\n",
        "# video_0_key, video_1_key = 0, 0\n",
        "# for time in times:\n",
        "#     if time[0] == video_0_speaker:\n",
        "#         video_0_dicts.append({'Key': video_0_key,\n",
        "#                               'Start': time[1][0],\n",
        "#                               'event': 'talking',\n",
        "#                               'End': time[1][0] + time[1][1],\n",
        "#                               'Duration': time[1][1]})\n",
        "#         video_0_key += 1\n",
        "\n",
        "#     else:\n",
        "#         video_1_dicts.append({'Key': video_1_key,\n",
        "#                               'Start': time[1][0],\n",
        "#                               'event': 'talking',\n",
        "#                               'End': time[1][0] + time[1][1],\n",
        "#                               'Duration': time[1][1]})\n",
        "#         video_1_key += 1"
      ],
      "metadata": {
        "id": "6_JWkXYhQDvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lip_height(lip):\n",
        "    sum=0\n",
        "    for i in [2, 3, 4]:\n",
        "        # distance between two near points up and down\n",
        "        distance = math.sqrt((lip[i][0] - lip[12-i][0])**2 +\n",
        "                             (lip[i][1] - lip[12-i][1])**2)\n",
        "        sum += distance\n",
        "    return sum / 3\n",
        "\n",
        "def get_mouth_height(top_lip, bottom_lip):\n",
        "    sum=0\n",
        "    for i in [8,9,10]:\n",
        "        # distance between two near points up and down\n",
        "        distance = math.sqrt((top_lip[i][0] - bottom_lip[18-i][0])**2 +\n",
        "                             (top_lip[i][1] - bottom_lip[18-i][1])**2)\n",
        "        sum += distance\n",
        "    return sum / 3\n",
        "\n",
        "def check_mouth_open(top_lip, bottom_lip):\n",
        "    top_lip_height = get_lip_height(top_lip)\n",
        "    bottom_lip_height = get_lip_height(bottom_lip)\n",
        "    mouth_height = get_mouth_height(top_lip, bottom_lip)\n",
        "\n",
        "    # if mouth is open more than lip height * ratio, return true.\n",
        "    ratio = 0.5\n",
        "    if mouth_height > min(top_lip_height, bottom_lip_height) * ratio:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "JRkU3jPHdp9U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change docker-compose.yml to gpu\n",
        "from face_recognition.face_recognition import api as face_recognition\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def is_mouth_open(face_landmarks):\n",
        "    top_lip = face_landmarks['top_lip']\n",
        "    bottom_lip = face_landmarks['bottom_lip']\n",
        "\n",
        "    top_lip_height = get_lip_height(top_lip)\n",
        "    bottom_lip_height = get_lip_height(bottom_lip)\n",
        "    mouth_height = get_mouth_height(top_lip, bottom_lip)\n",
        "\n",
        "    ratio = 0.5\n",
        "\n",
        "    if mouth_height > min(top_lip_height, bottom_lip_height) * ratio:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def detect_open_mouth(video, count_frames, speaker0_frames, speaker1_frames):\n",
        "    speaker0_frames_number, speaker1_frames_number = 0, 0\n",
        "    for current_frame in tqdm(range(count_frames)):\n",
        "        # Grab a single frame of video\n",
        "        ret, frame = video.read()\n",
        "\n",
        "        if not (speaker0_frames[0] <= current_frame <= speaker0_frames[1] or\n",
        "                speaker1_frames[0] <= current_frame <= speaker1_frames[1]):\n",
        "            continue\n",
        "\n",
        "        if current_frame % 5 != 0:\n",
        "            continue\n",
        "\n",
        "        # Find all the faces and face enqcodings in the frame of video\n",
        "        face_locations = face_recognition.face_locations(frame, model='cnn')\n",
        "        face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
        "        face_landmarks_list = face_recognition.face_landmarks(frame)\n",
        "\n",
        "        # Loop through each face in this frame of video\n",
        "        for face_landmarks in face_landmarks_list:\n",
        "            ret_mouth_open = is_mouth_open(face_landmarks)\n",
        "            if ret_mouth_open and speaker0_frames[0] <= current_frame <= speaker0_frames[1]:\n",
        "                speaker0_frames_number += 1\n",
        "            elif ret_mouth_open and speaker1_frames[0] <= current_frame <= speaker1_frames[1]:\n",
        "               speaker1_frames_number += 1\n",
        "\n",
        "    return (5 * speaker0_frames_number / (speaker0_frames[1] - speaker0_frames[0]),\n",
        "            5 * speaker1_frames_number / (speaker1_frames[1] - speaker1_frames[0]))"
      ],
      "metadata": {
        "id": "EkN1akcuCELw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_0 = cv2.VideoCapture('drive/MyDrive/studio2/speaker_0.mp4')\n",
        "video_1 = cv2.VideoCapture('drive/MyDrive/studio2/speaker_1.mp4')\n",
        "\n",
        "video_0_freq0, video_0_freq1 = detect_open_mouth(video_0,\n",
        "                                                 count_frames,\n",
        "                                                 max_speaker0_frames,\n",
        "                                                 max_speaker1_frames)\n",
        "\n",
        "video_1_freq0, video_1_freq1 = detect_open_mouth(video_1,\n",
        "                                                 count_frames,\n",
        "                                                 max_speaker0_frames,\n",
        "                                                 max_speaker1_frames)\n",
        "\n",
        "if max(video_0_freq0, video_0_freq1) > max(video_1_freq0, video_1_freq1):\n",
        "    if video_0_freq0 > video_0_freq1:\n",
        "        video_0_speaker = 'SPEAKER_00'\n",
        "        video_1_speaker = 'SPEAKER_01'\n",
        "\n",
        "    else:\n",
        "        video_0_speaker = 'SPEAKER_01'\n",
        "        video_1_speaker = 'SPEAKER_00'\n",
        "\n",
        "else:\n",
        "    if video_1_freq0 > video_1_freq1:\n",
        "        video_0_speaker = 'SPEAKER_01'\n",
        "        video_1_speaker = 'SPEAKER_00'\n",
        "\n",
        "    else:\n",
        "        video_0_speaker = 'SPEAKER_00'\n",
        "        video_1_speaker = 'SPEAKER_01'"
      ],
      "metadata": {
        "id": "bq8Fj6lseo4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c84982-4fa4-46d7-ef10-48c7b6b07953"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20429/20429 [10:29<00:00, 32.47it/s] \n",
            "100%|██████████| 20429/20429 [10:22<00:00, 32.81it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_0_dicts, video_1_dicts = [], []\n",
        "\n",
        "video_0_key, video_1_key = 0, 0\n",
        "for time in times:\n",
        "    if time[0] == video_0_speaker:\n",
        "        video_0_dicts.append({'Key': video_0_key,\n",
        "                              'Start': time[1][0],\n",
        "                              'event': 'talking',\n",
        "                              'End': time[1][0] + time[1][1],\n",
        "                              'Duration': time[1][1]})\n",
        "        video_0_key += 1\n",
        "\n",
        "    else:\n",
        "        video_1_dicts.append({'Key': video_1_key,\n",
        "                              'Start': time[1][0],\n",
        "                              'event': 'talking',\n",
        "                              'End': time[1][0] + time[1][1],\n",
        "                              'Duration': time[1][1]})\n",
        "        video_1_key += 1"
      ],
      "metadata": {
        "id": "10yrJE-SHrdH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/studio2/speaker_0.json\", \"w\") as final:\n",
        "    json.dump(video_0_dicts, final)\n",
        "\n",
        "with open(\"drive/MyDrive/studio2/speaker_1.json\", \"w\") as final:\n",
        "    json.dump(video_1_dicts, final)"
      ],
      "metadata": {
        "id": "MsPCth0OHwvG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERAeGybDOUaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}